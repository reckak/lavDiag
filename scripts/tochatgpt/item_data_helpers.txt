# -- Utility: safe builder + small helpers for mgcv::gam ---------------------

.fit_gam_models <- function(data, formula, family, args = list()) {
  # Lock fields we control to avoid accidental overrides
  args$formula <- NULL; args$data <- NULL; args$family <- NULL
  do.call(mgcv::gam, c(list(formula = formula, data = data, family = family), args))
}


.predict_gam_cols <- function(dat, fit, prefixes, suffix, level = 0.95) {
  # Predict on LINK scale to get valid SEs, then transform to RESPONSE via linkinv
  pr <- predict(fit, newdata = dat, type = "link", se.fit = TRUE)
  inv <- fit$family$linkinv
  z  <- stats::qnorm(1 - (1 - level)/2)
  mu_resp  <- inv(pr$fit)
  lwr_resp <- inv(pr$fit - z * pr$se.fit)
  upr_resp <- inv(pr$fit + z * pr$se.fit)
  out <- list(
    setNames(list(as.numeric(mu_resp)),  paste0(prefixes[1], suffix)),
    setNames(list(as.numeric(lwr_resp)), paste0(prefixes[2], suffix)),
    setNames(list(as.numeric(upr_resp)), paste0(prefixes[3], suffix))
  )
  as.data.frame(out)
}

# -- Helpers used in empirical curves ---------------------------------------
.rescale_01 <- function(x) {
  r <- range(x, na.rm = TRUE, finite = TRUE)
  if (!is.finite(r[1]) || !is.finite(r[2]) || r[2] == r[1]) return(rep(0.5, length(x)))
  (x - r[1]) / (r[2] - r[1])
}

.sv01 <- function(y01) {
  n <- sum(!is.na(y01))
  if (n <= 1L) return(y01)
  ((y01 * (n - 1)) + 0.5) / n
}

.clamp01 <- function(p, eps = 1e-6) pmin(pmax(p, eps), 1 - eps)

.linkinv_fun <- function(fam) if (!is.null(fam$linkinv)) fam$linkinv else stop("Family lacks linkinv.")



# Fast CIs for mgcv::ocat() by varying eta ± z * SE (thresholds treated as fixed)
.predict_ocat_ci_delta <- function(fit, newdata, level = 0.95, expected = TRUE) {
  stopifnot(inherits(fit, "gam"))

  # 1) Linear predictor and its SE for newdata
  pr <- predict(fit, newdata = newdata, type = "link", se.fit = TRUE)
  eta_hat <- drop(pr$fit)                # n
  se_hat  <- drop(pr$se.fit)             # n

  # 2) Ordered cut-points (fixed here)
  theta <- fit$family$getTheta(trans = TRUE)  # length R-1
  R <- length(theta) + 1L
  n <- length(eta_hat)

  # 3) Helper: from cumulative F (n x (R-1)) -> category probs (n x R)
  make_probs <- function(Fcum) {
    # P1 = F1; PR = 1 - F_{R-1}; middle by differences
    out <- matrix(NA_real_, nrow = nrow(Fcum), ncol = ncol(Fcum) + 1L)
    out[, 1] <- Fcum[, 1]
    if (ncol(Fcum) > 1L) {
      out[, 2:ncol(Fcum)] <- Fcum[, -ncol(Fcum), drop = FALSE] - Fcum[, -1, drop = FALSE]
    }
    out[, ncol(out)] <- 1 - Fcum[, ncol(Fcum)]
    out
  }

  # 4) Compute F(theta_r - eta) at eta_hat, and at eta_hat ± z*SE
  z <- stats::qnorm(1 - (1 - level) / 2)
  eta_low  <- eta_hat - z * se_hat
  eta_high <- eta_hat + z * se_hat

  # Build cumulative matrices via vectorized outer (n x (R-1))
  F_est  <- plogis(outer(eta_hat,  theta, function(e, th) th - e))
  F_low  <- plogis(outer(eta_low,  theta, function(e, th) th - e))
  F_high <- plogis(outer(eta_high, theta, function(e, th) th - e))

  P_est  <- make_probs(F_est)    # n x R
  P_low  <- make_probs(F_low)    # n x R
  P_high <- make_probs(F_high)   # n x R

  # Conservative bounds: take min/max across endpoints (eta_low vs eta_high)
  P_lwr <- pmin(P_low, P_high)
  P_upr <- pmax(P_low, P_high)

  # 5) Optional expected score E[Y] with coding 1..R
  if (expected) {
    scores <- matrix(rep(seq_len(R), each = n), nrow = n) # n x R
    E_est <- rowSums(P_est * scores)
    E_lwr <- rowSums(P_lwr * scores)
    E_upr <- rowSums(P_upr * scores)
  }

  # 6) Assemble a tidy data.frame
  out <- as.data.frame(
    do.call(
      cbind,
      c(
        lapply(seq_len(R), function(r) P_est[, r]),
        lapply(seq_len(R), function(r) P_lwr[, r]),
        lapply(seq_len(R), function(r) P_upr[, r])
      )
    )
  )
  est_names <- paste0("cat", seq_len(R), "_est")
  lwr_names <- paste0("cat", seq_len(R), "_lwr")
  upr_names <- paste0("cat", seq_len(R), "_upr")
  names(out) <- c(est_names, lwr_names, upr_names)

  if (expected) {
    out$E_est <- E_est
    out$E_lwr <- E_lwr
    out$E_upr <- E_upr
  }

  out
}

# ---- Helper: Ordered-category expected value + CIs via delta -----------------
#
# Computes per-category probabilities at the point estimate and (optionally)
# conservative bounds by evaluating at eta ± z*SE (then re-normalizing), and
# *delta-method* CIs for the expected score E[Y] coded 1..R.
#
# Notes:
# - mgcv::ocat() uses identity link; the latent comparison is theta_r - eta.
# - For CI on E[Y], we use Var(E) ≈ (dE/deta)^2 Var(eta_hat) with
#   dE/deta = Σ_r r * dP_r/deta and
#   dP_1/deta = -f(theta_1 - eta)
#   dP_r/deta = -f(theta_r - eta) + f(theta_{r-1} - eta) for 1<r<R
#   dP_R/deta =  f(theta_{R-1} - eta), where f(x) = F(x) * (1 - F(x)).
predict_ocat_ci_delta <- function(fit, newdata, level = 0.95, expected = TRUE) {
  stopifnot(inherits(fit, "gam"))

  # 1) Linear predictor and its SE for newdata
  pr <- predict(fit, newdata = newdata, type = "link", se.fit = TRUE)
  eta_hat <- as.numeric(pr$fit)                # n
  se_hat  <- pmax(0, as.numeric(pr$se.fit))    # n, guard against tiny negatives
  n <- length(eta_hat)

  # 2) Ordered cut-points
  theta <- fit$family$getTheta(trans = TRUE)   # length R-1
  R <- length(theta) + 1L

  # 3) Helpers ---------------------------------------------------------------
  # logistic CDF at (theta - eta)
  Fcum <- function(et) plogis(outer(et, theta, function(e, th) th - e))   # n x (R-1)
  # logistic pdf at (theta - eta)
  flog <- function(et) {
    F <- Fcum(et)
    F * (1 - F)  # n x (R-1)
  }
  make_probs <- function(Fc) {
    # Fc: n x (R-1) cumulative probs
    out <- matrix(NA_real_, nrow = nrow(Fc), ncol = ncol(Fc) + 1L) # n x R
    out[, 1] <- Fc[, 1]
    if (ncol(Fc) > 1L) {
      out[, 2:ncol(Fc)] <- Fc[, -ncol(Fc), drop = FALSE] - Fc[, -1, drop = FALSE]
    }
    out[, ncol(out)] <- 1 - Fc[, ncol(Fc)]
    # numerical cleanup
    out[out < 0] <- 0
    row_sums <- rowSums(out)
    row_sums[row_sums == 0] <- 1
    out / row_sums
  }

  # 4) Point probabilities
  Fc_est <- Fcum(eta_hat)
  P_est  <- make_probs(Fc_est)  # n x R

  # 5) Conservative envelope for per-category bounds (optional)
  z <- stats::qnorm(1 - (1 - level) / 2)
  Fc_low  <- Fcum(eta_hat - z * se_hat)
  Fc_high <- Fcum(eta_hat + z * se_hat)
  P_low  <- make_probs(Fc_low)
  P_high <- make_probs(Fc_high)
  # min/max across endpoints, then renormalize
  P_lwr <- pmin(P_low, P_high)
  P_upr <- pmax(P_low, P_high)
  P_lwr <- P_lwr / pmax(1, rowSums(P_lwr))
  P_upr <- P_upr / pmax(1, rowSums(P_upr))

  # 6) Expected value and delta-method CI on E[Y]
  if (expected) {
    # gradient dE/deta
    f_vals <- flog(eta_hat)              # n x (R-1)
    dP1 <- -f_vals[, 1, drop = TRUE]
    dPR <-  f_vals[, ncol(f_vals), drop = TRUE]
    dPmid <- if (R > 2L) f_vals[, 1:(R-2), drop = FALSE] - f_vals[, 2:(R-1), drop = FALSE] else NULL
    dP <- matrix(0, nrow = n, ncol = R)
    dP[, 1] <- dP1
    if (!is.null(dPmid)) dP[, 2:(R-1)] <- dPmid
    dP[, R] <- dPR

    r_idx <- matrix(rep(seq_len(R), each = n), nrow = n)
    dE <- rowSums(dP * r_idx)
    se_E <- abs(dE) * se_hat

    scores <- r_idx
    E_est <- rowSums(P_est * scores)
    E_lwr <- pmax(1, pmin(R, E_est - z * se_E))
    E_upr <- pmax(1, pmin(R, E_est + z * se_E))
  }

  # 7) Assemble output -------------------------------------------------------
  out <- as.data.frame(
    do.call(
      cbind,
      c(
        lapply(seq_len(R), function(r) P_est[, r]),
        lapply(seq_len(R), function(r) P_lwr[, r]),
        lapply(seq_len(R), function(r) P_upr[, r])
      )
    )
  )
  est_names <- paste0("cat", seq_len(R), "_est")
  lwr_names <- paste0("cat", seq_len(R), "_lwr")
  upr_names <- paste0("cat", seq_len(R), "_upr")
  names(out) <- c(est_names, lwr_names, upr_names)

  if (expected) {
    out$E_est <- E_est
    out$E_lwr <- E_lwr
    out$E_upr <- E_upr
  }

  out
}

